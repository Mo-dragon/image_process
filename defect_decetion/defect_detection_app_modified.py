#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¼ºé™·æ£€æµ‹ç”¨æˆ·ç«¯åº”ç”¨ç¨‹åºï¼ˆä¿®æ”¹ç‰ˆ - ç§»é™¤oilé€‰é¡¹å’ŒUIä¸­çš„ä¸‰åˆ†ç±»æ˜¾ç¤ºï¼‰
æ”¯æŒå•å¼ å›¾ç‰‡é¢„æµ‹ã€æ‰¹é‡å¤„ç†å’Œå¯è§†åŒ–åˆ†æ
"""

import os
import sys
import argparse
import json
import time
from pathlib import Path
import numpy as np
import cv2
from PIL import Image, ImageTk
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torchvision.models as models
import albumentations as A
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import pandas as pd

# GUIç›¸å…³
try:
    import tkinter as tk
    from tkinter import ttk, filedialog, messagebox

    GUI_AVAILABLE = True
except ImportError:
    GUI_AVAILABLE = False
    print("âš ï¸ GUIæ¨¡å—ä¸å¯ç”¨ï¼Œä»…æ”¯æŒå‘½ä»¤è¡Œæ¨¡å¼")


# ================================
# æ¨¡å‹ç»“æ„å®šä¹‰ï¼ˆä¸è®­ç»ƒä»£ç ä¿æŒä¸€è‡´ï¼‰
# ================================

class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æ¨¡å—"""

    def __init__(self, in_channels, reduction=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return self.sigmoid(out)


class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æ¨¡å—"""

    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        out = self.conv1(x_cat)
        return self.sigmoid(out)


class CBAM(nn.Module):
    """å·ç§¯å—æ³¨æ„åŠ›æ¨¡å—"""

    def __init__(self, in_channels, reduction=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttention(in_channels, reduction)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        out = x * self.channel_attention(x)
        out = out * self.spatial_attention(out)
        return out


class EnhancedResNetClassifier(nn.Module):
    """å¢å¼ºçš„ResNetåˆ†ç±»å™¨ï¼ˆä¿®æ”¹ä¸º2åˆ†ç±»ï¼šåˆ’ç—•å’Œæ–‘ç‚¹ï¼‰"""

    def __init__(self, num_classes=2, pretrained=True, use_attention=True):
        super(EnhancedResNetClassifier, self).__init__()

        self.backbone = models.resnet50(pretrained=pretrained)
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])

        self.feature_channels = 2048
        self.use_attention = use_attention

        if use_attention:
            self.attention = CBAM(self.feature_channels)

        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)

        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(self.feature_channels, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

        self.feature_maps = None
        self.attention_maps = None

    def forward(self, x):
        features = self.backbone(x)
        self.feature_maps = features.detach()

        if self.use_attention:
            attended_features = self.attention(features)
            self.attention_maps = attended_features.detach()
        else:
            attended_features = features

        pooled_features = self.global_avg_pool(attended_features)
        pooled_features = pooled_features.view(pooled_features.size(0), -1)
        output = self.classifier(pooled_features)

        return output


# ================================
# å›¾åƒé¢„å¤„ç†å‡½æ•°ï¼ˆç§»é™¤oilç›¸å…³ï¼‰
# ================================

def enhance_defect_visibility(image, defect_type=None):
    """å¢å¼ºç¼ºé™·ç‰¹å¾çš„å¯è§æ€§ï¼ˆç§»é™¤oilç±»å‹ï¼‰"""
    if defect_type == 'scratch':
        return enhance_scratch_defects(image)
    elif defect_type == 'stain':
        return enhance_stain_defects(image)
    else:
        # é€šç”¨å¢å¼º
        return enhance_general_defects(image)


def enhance_scratch_defects(image):
    """å¢å¼ºåˆ’ç—•ç‰¹å¾"""
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

    kernel_horizontal = np.array([[-1, -1, -1], [2, 2, 2], [-1, -1, -1]], dtype=np.float32)
    kernel_vertical = np.array([[-1, 2, -1], [-1, 2, -1], [-1, 2, -1]], dtype=np.float32)

    horizontal_edges = cv2.filter2D(gray, -1, kernel_horizontal)
    vertical_edges = cv2.filter2D(gray, -1, kernel_vertical)
    edges = np.maximum(horizontal_edges, vertical_edges)
    edges = np.clip(edges, 0, 255).astype(np.uint8)

    enhanced = image.copy()
    for i in range(3):
        channel = enhanced[:, :, i]
        mask = edges > 30
        channel[mask] = np.clip(channel[mask] * 1.3 + 20, 0, 255)
        enhanced[:, :, i] = channel

    return enhanced


def enhance_stain_defects(image):
    """å¢å¼ºæ–‘ç‚¹ç‰¹å¾"""
    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)

    lower_white = np.array([0, 0, 200])
    upper_white = np.array([180, 30, 255])
    white_mask = cv2.inRange(hsv, lower_white, upper_white)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    white_mask = cv2.morphologyEx(white_mask, cv2.MORPH_CLOSE, kernel)
    white_mask = cv2.morphologyEx(white_mask, cv2.MORPH_OPEN, kernel)

    enhanced = image.copy()
    enhanced[white_mask > 0] = np.clip(enhanced[white_mask > 0] * 1.2 + 15, 0, 255)
    enhanced = cv2.convertScaleAbs(enhanced, alpha=1.05, beta=5)

    return enhanced


def enhance_general_defects(image):
    """é€šç”¨ç¼ºé™·å¢å¼º"""
    # CLAHEå¢å¼º
    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    lab[:, :, 0] = clahe.apply(lab[:, :, 0])
    enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)

    # è½»å¾®é”åŒ–
    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])
    enhanced = cv2.filter2D(enhanced, -1, kernel)
    enhanced = np.clip(enhanced, 0, 255).astype(np.uint8)

    return enhanced


def get_prediction_transform():
    """è·å–é¢„æµ‹æ—¶çš„å›¾åƒå˜æ¢"""
    return A.Compose([
        A.Resize(224, 224),
        A.CLAHE(clip_limit=1.0, p=0.5),
    ])


# ================================
# 3åˆ†ç±»åˆ°2åˆ†ç±»çš„æ¦‚ç‡è½¬æ¢
# ================================

def convert_3class_to_2class(probabilities_3class):
    """
    å°†3åˆ†ç±»æ¦‚ç‡è½¬æ¢ä¸º2åˆ†ç±»æ¦‚ç‡
    åŸå§‹ï¼š[oil, scratch, stain]
    è½¬æ¢åï¼š[scratch, stain]
    oilçš„æ¦‚ç‡åˆ†é…ç»™scratchå’Œstain
    """
    if len(probabilities_3class) != 3:
        raise ValueError("è¾“å…¥æ¦‚ç‡æ•°ç»„é•¿åº¦å¿…é¡»ä¸º3")

    oil_prob, scratch_prob, stain_prob = probabilities_3class

    # å°†oilæ¦‚ç‡æŒ‰æ¯”ä¾‹åˆ†é…ç»™scratchå’Œstain
    # å¦‚æœscratchå’Œstainæ¦‚ç‡éƒ½ä¸º0ï¼Œåˆ™å¹³å‡åˆ†é…
    total_non_oil = scratch_prob + stain_prob

    if total_non_oil > 0:
        scratch_ratio = scratch_prob / total_non_oil
        stain_ratio = stain_prob / total_non_oil
    else:
        scratch_ratio = 0.5
        stain_ratio = 0.5

    # é‡æ–°åˆ†é…æ¦‚ç‡
    new_scratch_prob = scratch_prob + oil_prob * scratch_ratio
    new_stain_prob = stain_prob + oil_prob * stain_ratio

    # å½’ä¸€åŒ–ç¡®ä¿æ¦‚ç‡å’Œä¸º1
    total = new_scratch_prob + new_stain_prob
    if total > 0:
        new_scratch_prob /= total
        new_stain_prob /= total

    return np.array([new_scratch_prob, new_stain_prob])


# ================================
# ç¼ºé™·æ£€æµ‹å™¨ç±»ï¼ˆä¿®æ”¹ç‰ˆï¼‰
# ================================

class DefectDetector:
    """ç¼ºé™·æ£€æµ‹å™¨ä¸»ç±»ï¼ˆä¿®æ”¹ç‰ˆ - ç§»é™¤oilï¼‰"""

    def __init__(self, model_path, device=None):
        # æ›´æ–°ä¸º2åˆ†ç±»
        self.categories = ['scratch', 'stain']
        self.category_names = ['åˆ’ç—•', 'æ–‘ç‚¹']
        self.colors = ['#4ECDC4', '#45B7D1']

        # è®¾å¤‡è®¾ç½®
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device

        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½æ¨¡å‹
        self.model = self._load_model(model_path)
        self.transform = get_prediction_transform()

        print("âœ… ç¼ºé™·æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆï¼ˆ2åˆ†ç±»æ¨¡å¼ï¼šåˆ’ç—•/æ–‘ç‚¹ï¼‰")

    def _load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")

            # é¦–å…ˆåŠ è½½ä¸º3åˆ†ç±»æ¨¡å‹
            model_3class = EnhancedResNetClassifier(num_classes=3, pretrained=False, use_attention=True)

            # åŠ è½½æƒé‡
            checkpoint = torch.load(model_path, map_location=self.device)

            if 'model_state_dict' in checkpoint:
                model_3class.load_state_dict(checkpoint['model_state_dict'])
                print(f"ğŸ“Š åŸå§‹æ¨¡å‹æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {checkpoint.get('best_accuracy', 'N/A')}")
            else:
                model_3class.load_state_dict(checkpoint)

            model_3class.to(self.device)
            model_3class.eval()

            # æ£€æŸ¥åˆ†ç±»å™¨ç»“æ„ï¼Œæ‰¾åˆ°æ­£ç¡®çš„æœ€åä¸€å±‚
            print("ğŸ” æ£€æŸ¥æ¨¡å‹ç»“æ„...")
            classifier_layers = []
            for name, module in model_3class.classifier.named_children():
                classifier_layers.append((name, module))
                print(f"  {name}: {module}")

            # æ‰¾åˆ°æœ€åçš„Linearå±‚
            last_linear_name = None
            for name, module in reversed(classifier_layers):
                if isinstance(module, torch.nn.Linear):
                    last_linear_name = name
                    print(f"ğŸ“ æ‰¾åˆ°æœ€åçš„åˆ†ç±»å±‚: classifier.{name}")
                    break

            if last_linear_name is None:
                raise ValueError("æ— æ³•æ‰¾åˆ°åˆ†ç±»å™¨ä¸­çš„Linearå±‚")

            # åˆ›å»º2åˆ†ç±»æ¨¡å‹
            model_2class = EnhancedResNetClassifier(num_classes=2, pretrained=False, use_attention=True)

            # å¤åˆ¶é™¤æœ€ååˆ†ç±»å±‚å¤–çš„æ‰€æœ‰æƒé‡
            model_2class_dict = model_2class.state_dict()
            model_3class_dict = model_3class.state_dict()

            last_layer_weight_key = f'classifier.{last_linear_name}.weight'
            last_layer_bias_key = f'classifier.{last_linear_name}.bias'

            for key in model_2class_dict.keys():
                if key not in [last_layer_weight_key, last_layer_bias_key]:
                    if key in model_3class_dict:
                        model_2class_dict[key] = model_3class_dict[key]
                    else:
                        print(f"âš ï¸ æƒé‡é”®ä¸åŒ¹é…: {key}")

            # å¤„ç†æœ€åçš„åˆ†ç±»å±‚æƒé‡
            if last_layer_weight_key in model_3class_dict:
                old_weight = model_3class_dict[last_layer_weight_key]  # [3, 512]
                old_bias = model_3class_dict[last_layer_bias_key]  # [3]

                print(f"ğŸ”§ è½¬æ¢åˆ†ç±»å±‚æƒé‡: {old_weight.shape} -> [2, {old_weight.size(1)}]")

                # åˆ›å»ºæ–°çš„2åˆ†ç±»æƒé‡
                new_weight = torch.zeros(2, old_weight.size(1))
                new_bias = torch.zeros(2)

                # scratch (åŸç´¢å¼•1 -> æ–°ç´¢å¼•0)ï¼Œæ·»åŠ 30%çš„oilæƒé‡
                new_weight[0] = old_weight[1] + 0.3 * old_weight[0]
                new_bias[0] = old_bias[1] + 0.3 * old_bias[0]

                # stain (åŸç´¢å¼•2 -> æ–°ç´¢å¼•1)ï¼Œæ·»åŠ 70%çš„oilæƒé‡
                new_weight[1] = old_weight[2] + 0.7 * old_weight[0]
                new_bias[1] = old_bias[2] + 0.7 * old_bias[0]

                model_2class_dict[last_layer_weight_key] = new_weight
                model_2class_dict[last_layer_bias_key] = new_bias

                print(f"âœ… æƒé‡è½¬æ¢å®Œæˆ")
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°åˆ†ç±»å±‚æƒé‡: {last_layer_weight_key}")

            # åŠ è½½æƒé‡åˆ°2åˆ†ç±»æ¨¡å‹
            model_2class.load_state_dict(model_2class_dict)
            model_2class.to(self.device)
            model_2class.eval()

            # ä¿å­˜3åˆ†ç±»æ¨¡å‹ç”¨äºæ¦‚ç‡è½¬æ¢
            self.model_3class = model_3class

            print("âœ… æ¨¡å‹é€‚é…å®Œæˆï¼š3åˆ†ç±» -> 2åˆ†ç±»")
            return model_2class

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å¦‚æœé€‚é…å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨3åˆ†ç±»æ¨¡å‹
            print("ğŸ”„ å°è¯•ç›´æ¥ä½¿ç”¨3åˆ†ç±»æ¨¡å‹...")
            try:
                model = EnhancedResNetClassifier(num_classes=3, pretrained=False, use_attention=True)
                checkpoint = torch.load(model_path, map_location=self.device)

                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)

                model.to(self.device)
                model.eval()

                # ç›´æ¥ä½¿ç”¨3åˆ†ç±»æ¨¡å‹ï¼Œåœ¨é¢„æµ‹æ—¶è½¬æ¢æ¦‚ç‡
                self.model_3class = model
                self.use_3class_direct = True

                print("âœ… ä½¿ç”¨3åˆ†ç±»æ¨¡å‹ï¼Œé¢„æµ‹æ—¶è½¬æ¢æ¦‚ç‡")
                return model

            except Exception as e2:
                print(f"âŒ 3åˆ†ç±»æ¨¡å‹åŠ è½½ä¹Ÿå¤±è´¥: {e2}")
                raise e

    def preprocess_image(self, image_path, enhance_type=None):
        """é¢„å¤„ç†å•å¼ å›¾åƒ"""
        try:
            # è¯»å–å›¾åƒ
            image = Image.open(image_path).convert("RGB")
            image_np = np.array(image)

            # ç¼ºé™·å¢å¼ºï¼ˆç§»é™¤oilé€‰é¡¹ï¼‰
            if enhance_type and enhance_type != 'oil':
                image_np = enhance_defect_visibility(image_np, enhance_type)

            # æ•°æ®å¢å¼º
            augmented = self.transform(image=image_np)
            image_tensor = transforms.ToTensor()(augmented['image'])

            return image_tensor.unsqueeze(0), image_np

        except Exception as e:
            print(f"âŒ å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            raise

    def predict_single(self, image_path, enhance_type=None, return_attention=False):
        """é¢„æµ‹å•å¼ å›¾åƒï¼ˆä½¿ç”¨3åˆ†ç±»æ¨¡å‹ç„¶åè½¬æ¢ä¸º2åˆ†ç±»ï¼‰"""
        try:
            # é¢„å¤„ç†
            image_tensor, original_image = self.preprocess_image(image_path, enhance_type)
            image_tensor = image_tensor.to(self.device)

            # æ¨ç†
            with torch.no_grad():
                start_time = time.time()

                # ä½¿ç”¨3åˆ†ç±»æ¨¡å‹è·å–åŸå§‹é¢„æµ‹
                outputs_3class = self.model_3class(image_tensor)
                probabilities_3class = torch.softmax(outputs_3class, dim=1)[0].cpu().numpy()

                # è½¬æ¢ä¸º2åˆ†ç±»æ¦‚ç‡
                probabilities_2class = convert_3class_to_2class(probabilities_3class)

                # è·å–2åˆ†ç±»é¢„æµ‹ç»“æœ
                predicted_class = np.argmax(probabilities_2class)
                confidence = probabilities_2class[predicted_class]

                inference_time = time.time() - start_time

                # æ„å»ºç»“æœ
                result = {
                    'image_path': image_path,
                    'predicted_class': predicted_class,
                    'predicted_label': self.categories[predicted_class],
                    'predicted_name': self.category_names[predicted_class],
                    'confidence': confidence,
                    'probabilities': probabilities_2class,
                    'probabilities_3class_original': probabilities_3class,  # ä¿ç•™åŸå§‹3åˆ†ç±»æ¦‚ç‡ç”¨äºè°ƒè¯•
                    'inference_time': inference_time,
                    'original_image': original_image
                }

                # æ·»åŠ æ³¨æ„åŠ›å›¾
                if return_attention and hasattr(self.model_3class, 'attention_maps'):
                    if self.model_3class.attention_maps is not None:
                        attention_map = torch.mean(self.model_3class.attention_maps[0], dim=0).cpu().numpy()
                        attention_map = cv2.resize(attention_map, (224, 224))
                        attention_map = (attention_map - attention_map.min()) / (
                                attention_map.max() - attention_map.min())
                        result['attention_map'] = attention_map

                return result

        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            raise

    def predict_batch(self, image_paths, enhance_type=None, progress_callback=None):
        """æ‰¹é‡é¢„æµ‹"""
        results = []
        total_images = len(image_paths)

        print(f"ğŸ” å¼€å§‹æ‰¹é‡é¢„æµ‹ {total_images} å¼ å›¾åƒ...")

        for i, image_path in enumerate(image_paths):
            try:
                result = self.predict_single(image_path, enhance_type)
                results.append(result)

                if progress_callback:
                    progress_callback(i + 1, total_images)

                if (i + 1) % 10 == 0:
                    print(f"  å·²å¤„ç†: {i + 1}/{total_images}")

            except Exception as e:
                print(f"âš ï¸ è·³è¿‡å›¾åƒ {image_path}: {e}")
                continue

        print(f"âœ… æ‰¹é‡é¢„æµ‹å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(results)} å¼ å›¾åƒ")
        return results

    def generate_report(self, results, save_path=None):
        """ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š"""
        if not results:
            print("âš ï¸ æ²¡æœ‰é¢„æµ‹ç»“æœå¯ç”ŸæˆæŠ¥å‘Š")
            return None

        # ç»Ÿè®¡ä¿¡æ¯
        total_images = len(results)
        category_counts = {name: 0 for name in self.category_names}
        avg_confidence = 0
        avg_inference_time = 0

        for result in results:
            category_counts[result['predicted_name']] += 1
            avg_confidence += result['confidence']
            avg_inference_time += result['inference_time']

        avg_confidence /= total_images
        avg_inference_time /= total_images

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'model_type': '2-class (scratch/stain)',
            'total_images': total_images,
            'category_distribution': category_counts,
            'average_confidence': avg_confidence,
            'average_inference_time': avg_inference_time,
            'detailed_results': results
        }

        # ä¿å­˜æŠ¥å‘Š
        if save_path:
            try:
                with open(save_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2, default=str)
                print(f"ğŸ“Š æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
            except Exception as e:
                print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

        return report

    def visualize_prediction(self, result, save_path=None, show_attention=True):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        try:
            fig, axes = plt.subplots(1, 3 if show_attention and 'attention_map' in result else 2,
                                     figsize=(15, 5))

            # åŸå§‹å›¾åƒ
            axes[0].imshow(result['original_image'])
            axes[0].set_title('åŸå§‹å›¾åƒ')
            axes[0].axis('off')

            # é¢„æµ‹æ¦‚ç‡
            probs = result['probabilities']
            bars = axes[1].bar(self.category_names, probs, color=self.colors)
            axes[1].set_title(f'é¢„æµ‹ç»“æœ: {result["predicted_name"]} (ç½®ä¿¡åº¦: {result["confidence"]:.2%})')
            axes[1].set_ylabel('æ¦‚ç‡')
            axes[1].set_ylim(0, 1)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, prob in zip(bars, probs):
                height = bar.get_height()
                axes[1].text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                             f'{prob:.2%}', ha='center', va='bottom')

            # æ³¨æ„åŠ›å›¾
            if show_attention and 'attention_map' in result:
                attention_map = result['attention_map']
                im = axes[2].imshow(result['original_image'])
                axes[2].imshow(attention_map, alpha=0.6, cmap='hot')
                axes[2].set_title('æ³¨æ„åŠ›å›¾')
                axes[2].axis('off')

            plt.tight_layout()

            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"ğŸ“Š å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {save_path}")

            plt.show()

        except Exception as e:
            print(f"âŒ å¯è§†åŒ–å¤±è´¥: {e}")


# ================================
# GUIåº”ç”¨ç¨‹åºç±»ï¼ˆä¿®æ”¹ç‰ˆ - ç§»é™¤åŸå§‹ä¸‰åˆ†ç±»é€‰é¡¹ï¼‰
# ================================

if GUI_AVAILABLE:
    class DefectDetectionGUI:
        """ç¼ºé™·æ£€æµ‹GUIåº”ç”¨ï¼ˆä¿®æ”¹ç‰ˆ - ç§»é™¤åŸå§‹ä¸‰åˆ†ç±»æ˜¾ç¤ºé€‰é¡¹ï¼‰"""

        def __init__(self, detector):
            self.detector = detector
            self.current_image_path = None
            self.current_result = None

            # åˆ›å»ºä¸»çª—å£
            self.root = tk.Tk()
            self.root.title("æ™ºèƒ½ç¼ºé™·æ£€æµ‹ç³»ç»Ÿ (åˆ’ç—•/æ–‘ç‚¹)")
            self.root.geometry("1200x800")  # å¢å¤§çª—å£å°ºå¯¸
            self.root.configure(bg='#f0f0f0')

            # è®¾ç½®æ ·å¼å’Œå­—ä½“
            self.style = ttk.Style()
            self.style.theme_use('clam')

            # é…ç½®å­—ä½“å¤§å°
            self.configure_fonts()

            self.create_widgets()

        def configure_fonts(self):
            """é…ç½®ç•Œé¢å­—ä½“å¤§å°"""
            # å®šä¹‰å­—ä½“
            self.title_font = ('Arial', 18, 'bold')
            self.label_font = ('Arial', 12, 'bold')
            self.button_font = ('Arial', 11)
            self.text_font = ('Arial', 15)
            self.combo_font = ('Arial', 10)

            # é…ç½®ttkæ ·å¼çš„å­—ä½“
            self.style.configure('Title.TLabel', font=self.title_font)
            self.style.configure('Heading.TLabel', font=self.label_font)
            self.style.configure('Custom.TButton', font=self.button_font, padding=(10, 5))
            self.style.configure('Custom.TLabelframe.Label', font=self.label_font)
            self.style.configure('Custom.TCombobox', font=self.combo_font)
            self.style.configure('Custom.TCheckbutton', font=self.text_font)

        def create_widgets(self):
            """åˆ›å»ºGUIç»„ä»¶"""
            # ä¸»æ¡†æ¶
            main_frame = ttk.Frame(self.root, padding="15")
            main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))

            # æ ‡é¢˜
            title_label = ttk.Label(main_frame, text="æ™ºèƒ½ç¼ºé™·æ£€æµ‹ç³»ç»Ÿ (åˆ’ç—•/æ–‘ç‚¹åˆ†ç±»)",
                                    style='Title.TLabel')
            title_label.grid(row=0, column=0, columnspan=3, pady=(0, 25))

            # å·¦ä¾§é¢æ¿ - å›¾åƒæ˜¾ç¤º
            image_frame = ttk.LabelFrame(main_frame, text="å›¾åƒæ˜¾ç¤º", padding="15",
                                         style='Custom.TLabelframe')
            image_frame.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), padx=(0, 15))

            self.image_label = ttk.Label(image_frame, text="è¯·é€‰æ‹©å›¾åƒ",
                                         background='white', relief='sunken',
                                         font=self.text_font)
            self.image_label.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))

            # ä¸­é—´é¢æ¿ - æ§åˆ¶æŒ‰é’®
            control_frame = ttk.LabelFrame(main_frame, text="æ“ä½œæ§åˆ¶", padding="15",
                                           style='Custom.TLabelframe')
            control_frame.grid(row=1, column=1, sticky=(tk.W, tk.E, tk.N, tk.S), padx=8)

            # é€‰æ‹©å›¾åƒæŒ‰é’®
            select_btn = ttk.Button(control_frame, text="é€‰æ‹©å›¾åƒ",
                                    command=self.select_image, style='Custom.TButton')
            select_btn.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=(0, 8))

            # é¢„æµ‹æŒ‰é’®
            self.predict_btn = ttk.Button(control_frame, text="å¼€å§‹é¢„æµ‹",
                                          command=self.predict_image, state='disabled',
                                          style='Custom.TButton')
            self.predict_btn.grid(row=1, column=0, sticky=(tk.W, tk.E), pady=(0, 8))

            # æ‰¹é‡å¤„ç†æŒ‰é’®
            batch_btn = ttk.Button(control_frame, text="æ‰¹é‡å¤„ç†",
                                   command=self.batch_process, style='Custom.TButton')
            batch_btn.grid(row=2, column=0, sticky=(tk.W, tk.E), pady=(0, 8))

            # å³ä¾§é¢æ¿ - ç»“æœæ˜¾ç¤º
            result_frame = ttk.LabelFrame(main_frame, text="é¢„æµ‹ç»“æœ", padding="15",
                                          style='Custom.TLabelframe')
            result_frame.grid(row=1, column=2, sticky=(tk.W, tk.E, tk.N, tk.S), padx=(15, 0))

            # ç»“æœæ–‡æœ¬æ¡†
            self.result_text = tk.Text(result_frame, width=35, height=22, wrap=tk.WORD,
                                       font=self.text_font, bg='white', relief='sunken',
                                       borderwidth=2, padx=10, pady=10)
            scrollbar = ttk.Scrollbar(result_frame, orient=tk.VERTICAL, command=self.result_text.yview)
            self.result_text.configure(yscrollcommand=scrollbar.set)

            self.result_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
            scrollbar.grid(row=0, column=1, sticky=(tk.N, tk.S))

            # é…ç½®ç½‘æ ¼æƒé‡
            self.root.columnconfigure(0, weight=1)
            self.root.rowconfigure(0, weight=1)
            main_frame.columnconfigure(0, weight=2)
            main_frame.columnconfigure(1, weight=1)
            main_frame.columnconfigure(2, weight=1)
            main_frame.rowconfigure(1, weight=1)

            image_frame.columnconfigure(0, weight=1)
            image_frame.rowconfigure(0, weight=1)
            control_frame.columnconfigure(0, weight=1)
            result_frame.columnconfigure(0, weight=1)
            result_frame.rowconfigure(0, weight=1)

        def select_image(self):
            """é€‰æ‹©å›¾åƒæ–‡ä»¶"""
            file_path = filedialog.askopenfilename(
                title="é€‰æ‹©å›¾åƒæ–‡ä»¶",
                filetypes=[
                    ("å›¾åƒæ–‡ä»¶", "*.jpg *.jpeg *.png *.bmp *.tiff"),
                    ("JPEGæ–‡ä»¶", "*.jpg *.jpeg"),
                    ("PNGæ–‡ä»¶", "*.png"),
                    ("æ‰€æœ‰æ–‡ä»¶", "*.*")
                ]
            )

            if file_path:
                self.current_image_path = file_path
                self.display_image(file_path)
                self.predict_btn['state'] = 'normal'

        def display_image(self, image_path):
            """æ˜¾ç¤ºå›¾åƒ"""
            try:
                # åŠ è½½å¹¶è°ƒæ•´å›¾åƒå¤§å°
                image = Image.open(image_path)
                # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
                display_size = (400, 300)
                image.thumbnail(display_size, Image.Resampling.LANCZOS)

                # è½¬æ¢ä¸ºPhotoImage
                photo = ImageTk.PhotoImage(image)

                # æ›´æ–°æ ‡ç­¾
                self.image_label.configure(image=photo, text="")
                self.image_label.image = photo  # ä¿æŒå¼•ç”¨

            except Exception as e:
                messagebox.showerror("é”™è¯¯", f"æ— æ³•æ˜¾ç¤ºå›¾åƒ: {e}")

        def predict_image(self):
            """é¢„æµ‹å½“å‰å›¾åƒ"""
            if not self.current_image_path:
                messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆé€‰æ‹©å›¾åƒ")
                return

            try:
                # æ‰§è¡Œé¢„æµ‹ï¼ˆç§»é™¤ç¼ºé™·å¢å¼ºå’Œæ³¨æ„åŠ›å›¾é€‰é¡¹ï¼‰
                self.result_text.delete(1.0, tk.END)
                self.result_text.insert(tk.END, "ğŸ” æ­£åœ¨é¢„æµ‹...\n")
                self.root.update()

                result = self.detector.predict_single(
                    self.current_image_path,
                    enhance_type=None,
                    return_attention=False
                )

                self.current_result = result
                self.display_result(result)

            except Exception as e:
                messagebox.showerror("é”™è¯¯", f"é¢„æµ‹å¤±è´¥: {e}")

        def display_result(self, result):
            """æ˜¾ç¤ºé¢„æµ‹ç»“æœï¼ˆç§»é™¤åŸå§‹ä¸‰åˆ†ç±»æ¦‚ç‡æ˜¾ç¤ºï¼‰"""
            self.result_text.delete(1.0, tk.END)

            # åŸºæœ¬ä¿¡æ¯
            self.result_text.insert(tk.END, "ğŸ“Š é¢„æµ‹ç»“æœ\n")
            self.result_text.insert(tk.END, "=" * 30 + "\n")
            self.result_text.insert(tk.END, f"å›¾åƒ: {os.path.basename(result['image_path'])}\n")
            self.result_text.insert(tk.END, f"é¢„æµ‹ç±»åˆ«: {result['predicted_name']}\n")
            self.result_text.insert(tk.END, f"ç½®ä¿¡åº¦: {result['confidence']:.2%}\n")
            self.result_text.insert(tk.END, f"æ¨ç†è€—æ—¶: {result['inference_time']:.3f}ç§’\n\n")

            # 2åˆ†ç±»æ¦‚ç‡
            self.result_text.insert(tk.END, "ğŸ“ˆ ç±»åˆ«æ¦‚ç‡:\n")
            for i, (category, prob) in enumerate(zip(self.detector.category_names, result['probabilities'])):
                marker = "ğŸ‘‰ " if i == result['predicted_class'] else "   "
                self.result_text.insert(tk.END, f"{marker}{category}: {prob:.2%}\n")

        def batch_process(self):
            """æ‰¹é‡å¤„ç†"""
            folder_path = filedialog.askdirectory(title="é€‰æ‹©å›¾åƒæ–‡ä»¶å¤¹")
            if not folder_path:
                return

            # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
            image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
            image_paths = []

            for file_path in Path(folder_path).rglob('*'):
                if file_path.suffix.lower() in image_extensions:
                    image_paths.append(str(file_path))

            if not image_paths:
                messagebox.showwarning("è­¦å‘Š", "æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶")
                return

            # åˆ›å»ºè¿›åº¦çª—å£
            progress_window = tk.Toplevel(self.root)
            progress_window.title("æ‰¹é‡å¤„ç†è¿›åº¦")
            progress_window.geometry("450x180")
            progress_window.configure(bg='#f0f0f0')

            # è¿›åº¦çª—å£æ ‡é¢˜
            progress_title = ttk.Label(progress_window, text="æ‰¹é‡å¤„ç†è¿›åº¦",
                                       font=self.label_font)
            progress_title.pack(pady=15)

            progress_label = ttk.Label(progress_window, text="æ­£åœ¨å¤„ç†...",
                                       font=self.text_font)
            progress_label.pack(pady=5)

            progress_bar = ttk.Progressbar(progress_window, mode='determinate',
                                           length=350)
            progress_bar.pack(pady=15, padx=25, fill=tk.X)

            def update_progress(current, total):
                progress = (current / total) * 100
                progress_bar['value'] = progress
                progress_label.configure(text=f"æ­£åœ¨å¤„ç†: {current}/{total}")
                progress_window.update()

            try:
                # æ‰§è¡Œæ‰¹é‡é¢„æµ‹ï¼ˆç§»é™¤ç¼ºé™·å¢å¼ºé€‰é¡¹ï¼‰
                results = self.detector.predict_batch(
                    image_paths,
                    enhance_type=None,
                    progress_callback=update_progress
                )

                # ä¿å­˜ç»“æœ
                save_path = filedialog.asksaveasfilename(
                    title="ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ",
                    defaultextension=".json",
                    filetypes=[("JSONæ–‡ä»¶", "*.json"), ("æ‰€æœ‰æ–‡ä»¶", "*.*")]
                )

                if save_path:
                    report = self.detector.generate_report(results, save_path)

                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    stats_text = f"æ‰¹é‡å¤„ç†å®Œæˆ!\n\n"
                    stats_text += f"æ€»å›¾åƒæ•°: {len(results)}\n"
                    stats_text += f"å¹³å‡ç½®ä¿¡åº¦: {report['average_confidence']:.2%}\n"
                    stats_text += f"å¹³å‡å¤„ç†æ—¶é—´: {report['average_inference_time']:.3f}ç§’\n\n"
                    stats_text += "ç±»åˆ«åˆ†å¸ƒ:\n"
                    for category, count in report['category_distribution'].items():
                        stats_text += f"  {category}: {count}å¼ \n"

                    messagebox.showinfo("æ‰¹é‡å¤„ç†å®Œæˆ", stats_text)

                progress_window.destroy()

            except Exception as e:
                progress_window.destroy()
                messagebox.showerror("é”™è¯¯", f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}")

        def run(self):
            """è¿è¡ŒGUI"""
            self.root.mainloop()


# ================================
# å‘½ä»¤è¡Œæ¥å£å‡½æ•°ï¼ˆä¿®æ”¹ç‰ˆï¼‰
# ================================

def create_cli_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description="ç¼ºé™·æ£€æµ‹ç”¨æˆ·ç«¯åº”ç”¨ç¨‹åºï¼ˆä¿®æ”¹ç‰ˆ - ç§»é™¤oilé€‰é¡¹ï¼‰")
    parser.add_argument("--model", "-m", required=True, help="æ¨¡å‹æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--input", "-i", help="è¾“å…¥å›¾åƒè·¯å¾„æˆ–æ–‡ä»¶å¤¹")
    parser.add_argument("--output", "-o", help="è¾“å‡ºç»“æœè·¯å¾„")
    parser.add_argument("--enhance", "-e", choices=["scratch", "stain", "none"],
                        help="ç¼ºé™·å¢å¼ºç±»å‹ï¼ˆç§»é™¤oilé€‰é¡¹ï¼‰")
    parser.add_argument("--batch", "-b", action="store_true", help="æ‰¹é‡å¤„ç†æ¨¡å¼")
    parser.add_argument("--gui", "-g", action="store_true", help="å¯åŠ¨GUIç•Œé¢")
    parser.add_argument("--attention", "-a", action="store_true", help="ç”Ÿæˆæ³¨æ„åŠ›å›¾")
    parser.add_argument("--visualize", "-v", action="store_true", help="å¯è§†åŒ–é¢„æµ‹ç»“æœ")
    parser.add_argument("--device", "-d", choices=["cpu", "cuda"], help="æŒ‡å®šè®¾å¤‡")
    parser.add_argument("--show-3class", action="store_true", help="æ˜¾ç¤ºåŸå§‹3åˆ†ç±»æ¦‚ç‡")

    return parser


def run_cli_prediction(detector, args):
    """è¿è¡Œå‘½ä»¤è¡Œé¢„æµ‹"""
    if not args.input:
        print("âŒ è¯·æŒ‡å®šè¾“å…¥å›¾åƒè·¯å¾„æˆ–æ–‡ä»¶å¤¹")
        return

    input_path = Path(args.input)

    if not input_path.exists():
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return

    # å•å¼ å›¾åƒé¢„æµ‹
    if input_path.is_file():
        print(f"ğŸ” é¢„æµ‹å•å¼ å›¾åƒ: {input_path}")

        try:
            result = detector.predict_single(
                str(input_path),
                enhance_type=args.enhance,
                return_attention=args.attention
            )

            # æ˜¾ç¤ºç»“æœ
            print("\nğŸ“Š é¢„æµ‹ç»“æœ:")
            print("=" * 50)
            print(f"å›¾åƒ: {result['image_path']}")
            print(f"é¢„æµ‹ç±»åˆ«: {result['predicted_name']} ({result['predicted_label']})")
            print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f} ({result['confidence'] * 100:.2f}%)")
            print(f"æ¨ç†æ—¶é—´: {result['inference_time']:.3f}ç§’")

            print(f"\nå„ç±»åˆ«æ¦‚ç‡ (2åˆ†ç±»):")
            for i, (category, prob) in enumerate(zip(detector.category_names, result['probabilities'])):
                marker = "ğŸ‘‰" if i == result['predicted_class'] else "  "
                print(f"{marker} {category}: {prob:.4f} ({prob * 100:.2f}%)")

            # æ˜¾ç¤ºåŸå§‹3åˆ†ç±»æ¦‚ç‡ï¼ˆä»…åœ¨å‘½ä»¤è¡Œæ¨¡å¼ä¸‹ï¼‰
            if args.show_3class and 'probabilities_3class_original' in result:
                print(f"\nåŸå§‹3åˆ†ç±»æ¦‚ç‡:")
                original_categories = ['æ²¹æ±¡', 'åˆ’ç—•', 'æ–‘ç‚¹']
                for category, prob in zip(original_categories, result['probabilities_3class_original']):
                    print(f"   {category}: {prob:.4f} ({prob * 100:.2f}%)")
                print("   (æ²¹æ±¡æ¦‚ç‡å·²åˆå¹¶è‡³åˆ’ç—•å’Œæ–‘ç‚¹)")

            # å¯è§†åŒ–
            if args.visualize:
                save_vis_path = None
                if args.output:
                    save_vis_path = args.output.replace('.json', '_visualization.png')
                detector.visualize_prediction(result, save_vis_path, args.attention)

            # ä¿å­˜ç»“æœ
            if args.output:
                report = detector.generate_report([result], args.output)
                print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")

    # æ‰¹é‡é¢„æµ‹
    elif input_path.is_dir():
        print(f"ğŸ“ æ‰¹é‡é¢„æµ‹æ–‡ä»¶å¤¹: {input_path}")

        # æ”¶é›†å›¾åƒæ–‡ä»¶
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
        image_paths = []

        for file_path in input_path.rglob('*'):
            if file_path.suffix.lower() in image_extensions:
                image_paths.append(str(file_path))

        if not image_paths:
            print("âš ï¸ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶")
            return

        print(f"ğŸ“Š æ‰¾åˆ° {len(image_paths)} å¼ å›¾åƒ")

        try:
            # æ‰§è¡Œæ‰¹é‡é¢„æµ‹
            results = detector.predict_batch(image_paths, enhance_type=args.enhance)

            # ç”ŸæˆæŠ¥å‘Š
            output_path = args.output or f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            report = detector.generate_report(results, output_path)

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print("\nğŸ“ˆ æ‰¹é‡é¢„æµ‹ç»Ÿè®¡:")
            print("=" * 50)
            print(f"æ€»å›¾åƒæ•°: {report['total_images']}")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {report['average_confidence']:.4f} ({report['average_confidence'] * 100:.2f}%)")
            print(f"å¹³å‡æ¨ç†æ—¶é—´: {report['average_inference_time']:.3f}ç§’")

            print(f"\nç±»åˆ«åˆ†å¸ƒ:")
            for category, count in report['category_distribution'].items():
                percentage = count / report['total_images'] * 100
                print(f"  {category}: {count}å¼  ({percentage:.1f}%)")

        except Exception as e:
            print(f"âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = create_cli_parser()
    args = parser.parse_args()

    print("ğŸš€ ç¼ºé™·æ£€æµ‹ç”¨æˆ·ç«¯åº”ç”¨ç¨‹åºï¼ˆä¿®æ”¹ç‰ˆ - 2åˆ†ç±»ï¼šåˆ’ç—•/æ–‘ç‚¹ï¼‰")
    print("=" * 60)

    # è®¾å¤‡è®¾ç½®
    device = None
    if args.device:
        device = torch.device(args.device)

    try:
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        detector = DefectDetector(args.model, device=device)

        # GUIæ¨¡å¼
        if args.gui:
            if not GUI_AVAILABLE:
                print("âŒ GUIæ¨¡å—ä¸å¯ç”¨ï¼Œè¯·å®‰è£…tkinter")
                return

            print("ğŸ–¥ï¸ å¯åŠ¨GUIç•Œé¢...")
            app = DefectDetectionGUI(detector)
            app.run()

        # å‘½ä»¤è¡Œæ¨¡å¼
        else:
            run_cli_prediction(detector, args)

    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


# ================================
# ä¾¿æ·å‡½æ•°ï¼ˆä¿®æ”¹ç‰ˆï¼‰
# ================================

def quick_predict(model_path, image_path, enhance_type=None):
    """å¿«é€Ÿé¢„æµ‹å‡½æ•°ï¼ˆ2åˆ†ç±»ç‰ˆæœ¬ï¼‰"""
    detector = DefectDetector(model_path)
    result = detector.predict_single(image_path, enhance_type=enhance_type)

    print(f"é¢„æµ‹ç»“æœ: {result['predicted_name']} (ç½®ä¿¡åº¦: {result['confidence']:.2%})")

    # æ˜¾ç¤ºåŸå§‹3åˆ†ç±»æ¦‚ç‡
    if 'probabilities_3class_original' in result:
        print("åŸå§‹3åˆ†ç±»æ¦‚ç‡:", end=" ")
        original_categories = ['æ²¹æ±¡', 'åˆ’ç—•', 'æ–‘ç‚¹']
        for category, prob in zip(original_categories, result['probabilities_3class_original']):
            print(f"{category}:{prob:.2%}", end=" ")
        print("\n(æ²¹æ±¡æ¦‚ç‡å·²åˆå¹¶)")

    return result


def create_demo_script(model_path, test_images_dir):
    """åˆ›å»ºæ¼”ç¤ºè„šæœ¬ï¼ˆä¿®æ”¹ç‰ˆï¼‰"""
    demo_code = f'''#!/usr/bin/env python3
"""
ç¼ºé™·æ£€æµ‹æ¼”ç¤ºè„šæœ¬ï¼ˆä¿®æ”¹ç‰ˆ - 2åˆ†ç±»ï¼šåˆ’ç—•/æ–‘ç‚¹ï¼‰
"""

from defect_detection_app_modified import DefectDetector, quick_predict
import os

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "{model_path}"

# æµ‹è¯•å›¾åƒç›®å½•
TEST_IMAGES_DIR = "{test_images_dir}"

def demo_single_prediction():
    """æ¼”ç¤ºå•å¼ å›¾åƒé¢„æµ‹"""
    print("ğŸ” å•å¼ å›¾åƒé¢„æµ‹æ¼”ç¤ºï¼ˆ2åˆ†ç±»ç‰ˆæœ¬ï¼‰")
    print("-" * 40)

    # è·å–æµ‹è¯•å›¾åƒ
    test_images = []
    for file in os.listdir(TEST_IMAGES_DIR):
        if file.lower().endswith(('.jpg', '.jpeg', '.png')):
            test_images.append(os.path.join(TEST_IMAGES_DIR, file))

    if test_images:
        # é¢„æµ‹ç¬¬ä¸€å¼ å›¾åƒ
        result = quick_predict(MODEL_PATH, test_images[0])
        print(f"å›¾åƒ: {{os.path.basename(test_images[0])}}")
        print(f"é¢„æµ‹: {{result['predicted_name']}}")
        print(f"ç½®ä¿¡åº¦: {{result['confidence']:.2%}}")

def demo_batch_prediction():
    """æ¼”ç¤ºæ‰¹é‡é¢„æµ‹"""
    print("\\nğŸ“ æ‰¹é‡é¢„æµ‹æ¼”ç¤ºï¼ˆ2åˆ†ç±»ç‰ˆæœ¬ï¼‰")
    print("-" * 40)

    detector = DefectDetector(MODEL_PATH)

    # æ”¶é›†æ‰€æœ‰æµ‹è¯•å›¾åƒ
    image_paths = []
    for file in os.listdir(TEST_IMAGES_DIR):
        if file.lower().endswith(('.jpg', '.jpeg', '.png')):
            image_paths.append(os.path.join(TEST_IMAGES_DIR, file))

    if image_paths:
        results = detector.predict_batch(image_paths[:5])  # åªå¤„ç†å‰5å¼ 
        report = detector.generate_report(results, "demo_results_2class.json")

        print(f"å¤„ç†äº† {{len(results)}} å¼ å›¾åƒ")
        print("ç±»åˆ«åˆ†å¸ƒï¼ˆ2åˆ†ç±»ï¼‰:")
        for category, count in report['category_distribution'].items():
            print(f"  {{category}}: {{count}}å¼ ")

if __name__ == "__main__":
    demo_single_prediction()
    demo_batch_prediction()
'''

    with open("demo_defect_detection_2class.py", "w", encoding="utf-8") as f:
        f.write(demo_code)

    print("ğŸ“ æ¼”ç¤ºè„šæœ¬å·²ç”Ÿæˆ: demo_defect_detection_2class.py")


# ================================
# æ¨¡å‹è½¬æ¢å·¥å…·
# ================================

def convert_3class_to_2class_model(model_3class_path, model_2class_path):
    """
    å°†3åˆ†ç±»æ¨¡å‹è½¬æ¢ä¸º2åˆ†ç±»æ¨¡å‹
    è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„è½¬æ¢å·¥å…·å‡½æ•°
    """
    try:
        print("ğŸ”„ å¼€å§‹æ¨¡å‹è½¬æ¢...")

        # åŠ è½½3åˆ†ç±»æ¨¡å‹
        model_3class = EnhancedResNetClassifier(num_classes=3, pretrained=False, use_attention=True)
        checkpoint = torch.load(model_3class_path, map_location='cpu')

        if 'model_state_dict' in checkpoint:
            model_3class.load_state_dict(checkpoint['model_state_dict'])
            original_accuracy = checkpoint.get('best_accuracy', 'N/A')
        else:
            model_3class.load_state_dict(checkpoint)
            original_accuracy = 'N/A'

        print(f"âœ… åŸå§‹3åˆ†ç±»æ¨¡å‹åŠ è½½å®Œæˆï¼Œæœ€ä½³å‡†ç¡®ç‡: {original_accuracy}")

        # åˆ›å»º2åˆ†ç±»æ¨¡å‹
        model_2class = EnhancedResNetClassifier(num_classes=2, pretrained=False, use_attention=True)

        # å¤åˆ¶æƒé‡
        model_2class_dict = model_2class.state_dict()
        model_3class_dict = model_3class.state_dict()

        for key in model_2class_dict.keys():
            if 'classifier.5' not in key:  # è·³è¿‡æœ€åçš„åˆ†ç±»å±‚
                model_2class_dict[key] = model_3class_dict[key]

        # å¤„ç†åˆ†ç±»å±‚æƒé‡
        if 'classifier.5.weight' in model_3class_dict:
            old_weight = model_3class_dict['classifier.5.weight']  # [3, 512]
            old_bias = model_3class_dict['classifier.5.bias']  # [3]

            # åˆ›å»ºæ–°çš„2åˆ†ç±»æƒé‡
            new_weight = torch.zeros(2, old_weight.size(1))
            new_bias = torch.zeros(2)

            # scratch (åŸç´¢å¼•1 -> æ–°ç´¢å¼•0)ï¼Œæ·»åŠ 30%çš„oilæƒé‡
            new_weight[0] = old_weight[1] + 0.3 * old_weight[0]
            new_bias[0] = old_bias[1] + 0.3 * old_bias[0]

            # stain (åŸç´¢å¼•2 -> æ–°ç´¢å¼•1)ï¼Œæ·»åŠ 70%çš„oilæƒé‡
            new_weight[1] = old_weight[2] + 0.7 * old_weight[0]
            new_bias[1] = old_bias[2] + 0.7 * old_bias[0]

            model_2class_dict['classifier.5.weight'] = new_weight
            model_2class_dict['classifier.5.bias'] = new_bias

        model_2class.load_state_dict(model_2class_dict)

        # ä¿å­˜2åˆ†ç±»æ¨¡å‹
        save_checkpoint = {
            'model_state_dict': model_2class.state_dict(),
            'num_classes': 2,
            'categories': ['scratch', 'stain'],
            'category_names': ['åˆ’ç—•', 'æ–‘ç‚¹'],
            'converted_from': model_3class_path,
            'original_3class_accuracy': original_accuracy,
            'conversion_time': datetime.now().isoformat()
        }

        torch.save(save_checkpoint, model_2class_path)
        print(f"âœ… 2åˆ†ç±»æ¨¡å‹å·²ä¿å­˜è‡³: {model_2class_path}")

        return model_2class_path

    except Exception as e:
        print(f"âŒ æ¨¡å‹è½¬æ¢å¤±è´¥: {e}")
        raise


# ================================
# ä½¿ç”¨ç¤ºä¾‹
# ================================

if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œæ˜¾ç¤ºä½¿ç”¨å¸®åŠ©
    if len(sys.argv) == 1:
        print("ğŸš€ ç¼ºé™·æ£€æµ‹ç”¨æˆ·ç«¯åº”ç”¨ç¨‹åºï¼ˆä¿®æ”¹ç‰ˆ - 2åˆ†ç±»ï¼šåˆ’ç—•/æ–‘ç‚¹ï¼‰")
        print("=" * 60)
        print()
        print("ä½¿ç”¨æ–¹æ³•:")
        print("1. GUIæ¨¡å¼:")
        print("   python defect_detection_app_modified.py --model best_enhanced_model.pth --gui")
        print()
        print("2. å•å¼ å›¾åƒé¢„æµ‹:")
        print("   python defect_detection_app_modified.py --model model.pth --input image.jpg")
        print()
        print("3. æ‰¹é‡é¢„æµ‹:")
        print(
            "   python defect_detection_app_modified.py --model model.pth --input images_folder/ --output results.json")
        print()

        print()
        print("å‚æ•°è¯´æ˜:")
        print("  --model, -m     : æ¨¡å‹æ–‡ä»¶è·¯å¾„ (å¿…éœ€)")
        print("  --input, -i     : è¾“å…¥å›¾åƒæˆ–æ–‡ä»¶å¤¹è·¯å¾„")
        print("  --output, -o    : è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„")
        print("  --gui, -g       : å¯åŠ¨GUIç•Œé¢")
        print("  --attention, -a : ç”Ÿæˆæ³¨æ„åŠ›å›¾")
        print("  --visualize, -v : å¯è§†åŒ–é¢„æµ‹ç»“æœ")
        print("  --device, -d    : æŒ‡å®šè®¾å¤‡ (cpu/cuda)")

        print()
        print("ğŸ”§ å·¥å…·å‡½æ•°:")
        print("  - quick_predict(): å¿«é€Ÿé¢„æµ‹å•å¼ å›¾åƒ")
        print("  - convert_3class_to_2class_model(): æ¨¡å‹æ ¼å¼è½¬æ¢")
        print("  - create_demo_script(): ç”Ÿæˆæ¼”ç¤ºè„šæœ¬")
        print()
        print("ğŸ¯ GUIç•Œé¢ç‰¹ç‚¹:")
        print("  - ç®€æ´çš„2åˆ†ç±»ç»“æœæ˜¾ç¤ºï¼ˆä»…æ˜¾ç¤ºåˆ’ç—•å’Œæ–‘ç‚¹æ¦‚ç‡ï¼‰")

        print("  - ä¿ç•™æ³¨æ„åŠ›å›¾æ˜¾ç¤ºåŠŸèƒ½")
        print("  - æ”¯æŒç¼ºé™·å¢å¼ºå¤„ç†")
    else:
        main()