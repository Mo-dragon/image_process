import os
import numpy as np
import random
import cv2
from collections import defaultdict
from PIL import Image, ImageEnhance
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.models as models
import albumentations as A
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
from tqdm import tqdm
import csv


# ================================
# å¢å¼ºçš„æ•°æ®é›†ç±» - ä¸“æ³¨ç¼ºé™·åŒºåŸŸ
# ================================

class DefectFocusedDataset(Dataset):
    """ä¸“æ³¨ç¼ºé™·åŒºåŸŸçš„æ•°æ®é›†"""

    def __init__(self, image_paths, transform=None, focus_on_defects=True):
        self.image_paths = image_paths
        self.transform = transform
        self.focus_on_defects = focus_on_defects
        self.categories = ['oil', 'scratch', 'stain']

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert("RGB")
        image_np = np.array(image)

        # è·å–æ ‡ç­¾
        label = self._get_label_from_folder(image_path)

        # æ ¹æ®ç¼ºé™·ç±»å‹è¿›è¡Œç‰¹å®šçš„é¢„å¤„ç†
        if self.focus_on_defects:
            image_np = self._enhance_defect_visibility(image_np, label)

        # åº”ç”¨æ•°æ®å¢å¼º
        if self.transform:
            if isinstance(self.transform, A.Compose):
                augmented = self.transform(image=image_np)
                image_np = augmented['image']

        # è½¬æ¢ä¸ºå¼ é‡
        image = transforms.ToTensor()(image_np)

        return image, label, image_path

    def _get_label_from_folder(self, image_path):
        folder_name = os.path.basename(os.path.dirname(image_path))
        if folder_name in self.categories:
            return self.categories.index(folder_name)
        else:
            raise ValueError(f"Unexpected folder name: {folder_name}")

    def _enhance_defect_visibility(self, image, label):
        """æ ¹æ®ç¼ºé™·ç±»å‹å¢å¼ºç‰¹å®šç‰¹å¾çš„å¯è§æ€§"""

        if label == 0:  # oil - æ²¹æ±¡ï¼šå¢å¼ºé€æ˜åº¦å·®å¼‚
            return self._enhance_oil_defects(image)
        elif label == 1:  # scratch - åˆ’ç—•ï¼šå¢å¼ºç»†é•¿æµ…ç™½ç—•
            return self._enhance_scratch_defects(image)
        elif label == 2:  # stain - æ–‘ç‚¹ï¼šå¢å¼ºç™½è‰²å°ç‚¹
            return self._enhance_stain_defects(image)
        else:
            return image

    def _enhance_oil_defects(self, image):
        """å¢å¼ºæ²¹æ±¡ç‰¹å¾ï¼šè¾ƒå¤§åŒºåŸŸé€æ˜çŠ¶"""
        # è½¬æ¢ä¸ºLABè‰²å½©ç©ºé—´ï¼ŒLé€šé“å¯¹äº®åº¦æ•æ„Ÿ
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        l_channel = lab[:, :, 0]

        # å¢å¼ºäº®åº¦å¯¹æ¯”åº¦ï¼Œçªå‡ºé€æ˜åŒºåŸŸ
        l_enhanced = cv2.equalizeHist(l_channel)
        lab[:, :, 0] = l_enhanced

        # è½¬å›RGB
        enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)

        # é¢å¤–çš„å¯¹æ¯”åº¦å¢å¼º
        enhanced = cv2.convertScaleAbs(enhanced, alpha=1.1, beta=10)

        return enhanced

    def _enhance_scratch_defects(self, image):
        """å¢å¼ºåˆ’ç—•ç‰¹å¾ï¼šç»†é•¿æµ…ç™½ç—•"""
        # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œè¾¹ç¼˜æ£€æµ‹
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

        # ä½¿ç”¨å®šå‘æ»¤æ³¢å™¨æ£€æµ‹çº¿æ€§ç‰¹å¾
        kernel_horizontal = np.array([[-1, -1, -1],
                                      [2, 2, 2],
                                      [-1, -1, -1]], dtype=np.float32)

        kernel_vertical = np.array([[-1, 2, -1],
                                    [-1, 2, -1],
                                    [-1, 2, -1]], dtype=np.float32)

        # æ£€æµ‹æ°´å¹³å’Œå‚ç›´çº¿æ€§ç‰¹å¾
        horizontal_edges = cv2.filter2D(gray, -1, kernel_horizontal)
        vertical_edges = cv2.filter2D(gray, -1, kernel_vertical)

        # åˆå¹¶è¾¹ç¼˜æ£€æµ‹ç»“æœ
        edges = np.maximum(horizontal_edges, vertical_edges)
        edges = np.clip(edges, 0, 255).astype(np.uint8)

        # å°†è¾¹ç¼˜ä¿¡æ¯èåˆå›åŸå›¾åƒ
        enhanced = image.copy()
        for i in range(3):  # RGBä¸‰ä¸ªé€šé“
            channel = enhanced[:, :, i]
            # å¢å¼ºæœ‰è¾¹ç¼˜çš„åŒºåŸŸ
            mask = edges > 30
            channel[mask] = np.clip(channel[mask] * 1.3 + 20, 0, 255)
            enhanced[:, :, i] = channel

        return enhanced

    def _enhance_stain_defects(self, image):
        """å¢å¼ºæ–‘ç‚¹ç‰¹å¾ï¼šç™½è‰²å°ç‚¹"""
        # è½¬æ¢ä¸ºHSVè‰²å½©ç©ºé—´
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)

        # æ£€æµ‹ç™½è‰²åŒºåŸŸï¼ˆé«˜äº®åº¦ï¼Œä½é¥±å’Œåº¦ï¼‰
        lower_white = np.array([0, 0, 200])
        upper_white = np.array([180, 30, 255])
        white_mask = cv2.inRange(hsv, lower_white, upper_white)

        # å½¢æ€å­¦æ“ä½œå»é™¤å™ªå£°ï¼Œä¿ç•™å°çš„ç™½è‰²æ–‘ç‚¹
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        white_mask = cv2.morphologyEx(white_mask, cv2.MORPH_CLOSE, kernel)
        white_mask = cv2.morphologyEx(white_mask, cv2.MORPH_OPEN, kernel)

        # å¢å¼ºç™½è‰²æ–‘ç‚¹åŒºåŸŸ
        enhanced = image.copy()
        enhanced[white_mask > 0] = np.clip(enhanced[white_mask > 0] * 1.2 + 15, 0, 255)

        # æ•´ä½“è½»å¾®å¢å¼ºå¯¹æ¯”åº¦
        enhanced = cv2.convertScaleAbs(enhanced, alpha=1.05, beta=5)

        return enhanced


# ================================
# æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—
# ================================

class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æ¨¡å—"""

    def __init__(self, in_channels, reduction=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return self.sigmoid(out)


class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æ¨¡å—"""

    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        out = self.conv1(x_cat)
        return self.sigmoid(out)


class CBAM(nn.Module):
    """å·ç§¯å—æ³¨æ„åŠ›æ¨¡å— (Convolutional Block Attention Module)"""

    def __init__(self, in_channels, reduction=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttention(in_channels, reduction)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        # é€šé“æ³¨æ„åŠ›
        out = x * self.channel_attention(x)
        # ç©ºé—´æ³¨æ„åŠ›
        out = out * self.spatial_attention(out)
        return out


# ================================
# å¢å¼ºçš„ResNetåˆ†ç±»å™¨
# ================================

class EnhancedResNetClassifier(nn.Module):
    """å¢å¼ºçš„ResNetåˆ†ç±»å™¨ï¼Œé›†æˆæ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, num_classes=3, pretrained=True, use_attention=True):
        super(EnhancedResNetClassifier, self).__init__()

        # ä½¿ç”¨ResNet50ä½œä¸ºä¸»å¹²ç½‘ç»œ
        self.backbone = models.resnet50(pretrained=pretrained)

        # ç§»é™¤åŸå§‹çš„å…¨è¿æ¥å±‚å’Œå¹³å‡æ± åŒ–å±‚
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])

        # è·å–ç‰¹å¾å›¾çš„é€šé“æ•°
        self.feature_channels = 2048

        # æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
        self.use_attention = use_attention
        if use_attention:
            self.attention = CBAM(self.feature_channels)

        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)

        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(self.feature_channels, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

        # ç‰¹å¾å›¾å¯è§†åŒ–é’©å­ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        self.feature_maps = None
        self.attention_maps = None

    def forward(self, x):
        # æå–ç‰¹å¾
        features = self.backbone(x)  # [B, 2048, H, W]

        # ä¿å­˜ç‰¹å¾å›¾ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        self.feature_maps = features.detach()

        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
        if self.use_attention:
            attended_features = self.attention(features)
            self.attention_maps = attended_features.detach()
        else:
            attended_features = features

        # å…¨å±€å¹³å‡æ± åŒ–
        pooled_features = self.global_avg_pool(attended_features)
        pooled_features = pooled_features.view(pooled_features.size(0), -1)

        # åˆ†ç±»
        output = self.classifier(pooled_features)

        return output


# ================================
# ç„¦ç‚¹æŸå¤±å‡½æ•°
# ================================

class FocalLoss(nn.Module):
    """
    ç„¦ç‚¹æŸå¤±å‡½æ•°ï¼Œç”¨äºå¤„ç†å›°éš¾æ ·æœ¬å’Œç±»åˆ«ä¸å¹³è¡¡
    ç€é‡å…³æ³¨éš¾ä»¥åˆ†ç±»çš„æ ·æœ¬
    """

    def __init__(self, alpha=1.0, gamma=2.0, num_classes=3):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.num_classes = num_classes

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()


# ================================
# æ•°æ®å¢å¼ºç­–ç•¥ - ä¸“æ³¨ç¼ºé™·åŒºåŸŸ
# ================================

def get_defect_focused_augmentations(is_training=True):
    """è·å–ä¸“æ³¨ç¼ºé™·åŒºåŸŸçš„æ•°æ®å¢å¼ºç­–ç•¥"""

    if is_training:
        return A.Compose([
            # åŸºç¡€å˜æ¢
            A.Resize(224, 224),

            # å‡ ä½•å˜æ¢ - ä¿æŒç¼ºé™·å½¢çŠ¶ç‰¹å¾
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.3),
            A.Rotate(limit=45, p=0.6),
            A.ShiftScaleRotate(
                shift_limit=0.1,
                scale_limit=0.1,
                rotate_limit=45,
                p=0.5
            ),

            # å¼ºåº¦å’Œå¯¹æ¯”åº¦å˜æ¢ - å¢å¼ºç¼ºé™·å¯è§æ€§
            A.RandomBrightnessContrast(
                brightness_limit=0.2,
                contrast_limit=0.3,
                p=0.7
            ),
            A.CLAHE(clip_limit=2.0, p=0.5),  # é™åˆ¶å¯¹æ¯”åº¦è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡
            A.RandomGamma(gamma_limit=(80, 120), p=0.4),

            # å™ªå£°å’Œæ¨¡ç³Š - æé«˜é²æ£’æ€§
            A.GaussNoise(var_limit=(10.0, 40.0), p=0.3),
            A.GaussianBlur(blur_limit=3, p=0.2),

            # é¢œè‰²å˜æ¢ - é€‚åº”ä¸åŒå…‰ç…§æ¡ä»¶
            A.HueSaturationValue(
                hue_shift_limit=10,
                sat_shift_limit=20,
                val_shift_limit=10,
                p=0.4
            ),

            # ç»†èŠ‚å¢å¼ºå˜æ¢
            A.Sharpen(alpha=(0.2, 0.5), lightness=(0.8, 1.2), p=0.3),
            A.UnsharpMask(blur_limit=3, p=0.2),
        ])
    else:
        return A.Compose([
            A.Resize(224, 224),
            # éªŒè¯å’Œæµ‹è¯•æ—¶å¯ä»¥æ·»åŠ è½»å¾®çš„TTAï¼ˆæµ‹è¯•æ—¶æ•°æ®å¢å¼ºï¼‰
            # A.CLAHE(clip_limit=1.0, p=1.0),
        ])


# ================================
# æ”¹è¿›çš„è®­ç»ƒå‡½æ•°
# ================================

def train_enhanced_classification(model, train_loader, val_loader, device, num_epochs=50):
    """è®­ç»ƒå¢å¼ºçš„åˆ†ç±»æ¨¡å‹"""

    # ä½¿ç”¨ç„¦ç‚¹æŸå¤±
    criterion = FocalLoss(alpha=1.0, gamma=2.0, num_classes=3)

    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œæ›´å¥½çš„æƒé‡è¡°å‡
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)

    # ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )

    best_accuracy = 0.0
    train_losses = []
    val_accuracies = []
    patience_counter = 0
    patience = 10

    print("ğŸš€ å¼€å§‹å¢å¼ºåˆ†ç±»æ¨¡å‹è®­ç»ƒ...")

    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs}")

        for batch_idx, (images, labels, _) in enumerate(pbar):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()

            # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            current_acc = train_correct / train_total
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_acc:.4f}',
                'LR': f'{optimizer.param_groups[0]["lr"]:.6f}'
            })

        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = train_correct / train_total

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_correct = 0
        val_total = 0
        val_loss = 0.0

        with torch.no_grad():
            for images, labels, _ in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        val_accuracy = val_correct / val_total
        avg_val_loss = val_loss / len(val_loader)

        train_losses.append(avg_train_loss)
        val_accuracies.append(val_accuracy)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        print(f"\nEpoch {epoch + 1}/{num_epochs}:")
        print(f"  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.4f}")
        print(f"  éªŒè¯æŸå¤±: {avg_val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
        print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_accuracy': best_accuracy,
            }, "best_enhanced_model.pth")
            print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f})")
            patience_counter = 0
        else:
            patience_counter += 1

        # æ—©åœæ£€æŸ¥
        if patience_counter >= patience:
            print(f"  â° éªŒè¯å‡†ç¡®ç‡è¿ç»­ {patience} ä¸ªepochæœªæ”¹å–„ï¼Œæå‰åœæ­¢è®­ç»ƒ")
            break

        print()

    return train_losses, val_accuracies, best_accuracy


# ================================
# ç‰¹å¾å¯è§†åŒ–å‡½æ•°
# ================================

def visualize_attention_maps(model, dataloader, device, save_dir="attention_visualizations"):
    """å¯è§†åŒ–æ³¨æ„åŠ›å›¾"""

    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    model.eval()
    categories = ['oil', 'scratch', 'stain']
    category_names = ['æ²¹æ±¡', 'åˆ’ç—•', 'æ–‘ç‚¹']

    with torch.no_grad():
        for batch_idx, (images, labels, image_paths) in enumerate(dataloader):
            if batch_idx >= 3:  # åªå¯è§†åŒ–å‰å‡ ä¸ªbatch
                break

            images = images.to(device)
            outputs = model(images)

            # è·å–æ³¨æ„åŠ›å›¾
            if hasattr(model, 'attention_maps') and model.attention_maps is not None:
                attention_maps = model.attention_maps.cpu()

                for i in range(min(4, images.size(0))):  # æ¯ä¸ªbatchæœ€å¤š4å¼ å›¾
                    # åŸå§‹å›¾åƒ
                    original_img = images[i].cpu().permute(1, 2, 0).numpy()
                    original_img = (original_img - original_img.min()) / (original_img.max() - original_img.min())

                    # æ³¨æ„åŠ›å›¾ (å–é€šé“å¹³å‡)
                    attention_map = torch.mean(attention_maps[i], dim=0).numpy()
                    attention_map = cv2.resize(attention_map, (224, 224))
                    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())

                    # åˆ›å»ºçƒ­åŠ›å›¾
                    plt.figure(figsize=(12, 4))

                    plt.subplot(1, 3, 1)
                    plt.imshow(original_img)
                    plt.title('åŸå§‹å›¾åƒ')
                    plt.axis('off')

                    plt.subplot(1, 3, 2)
                    plt.imshow(attention_map, cmap='hot')
                    plt.title('æ³¨æ„åŠ›å›¾')
                    plt.axis('off')

                    plt.subplot(1, 3, 3)
                    plt.imshow(original_img)
                    plt.imshow(attention_map, alpha=0.6, cmap='hot')
                    plt.title('å åŠ æ˜¾ç¤º')
                    plt.axis('off')

                    # ä¿å­˜å›¾åƒ
                    label = labels[i].item()
                    filename = f"attention_batch{batch_idx}_img{i}_{category_names[label]}.png"
                    plt.tight_layout()
                    plt.savefig(os.path.join(save_dir, filename), dpi=150, bbox_inches='tight')
                    plt.close()

    print(f"ğŸ“Š æ³¨æ„åŠ›å¯è§†åŒ–å›¾å·²ä¿å­˜åˆ°: {save_dir}")


# ================================
# æ•°æ®é›†åˆ’åˆ†å‡½æ•°ï¼ˆç›´æ¥æ•´åˆï¼‰
# ================================

def create_balanced_dataset_split(base_dir, test_samples_per_class=30, train_ratio=0.8, random_seed=42):
    """
    åˆ›å»ºå¹³è¡¡çš„æ•°æ®é›†åˆ’åˆ†ï¼Œç¡®ä¿æµ‹è¯•é›†æ¯ä¸ªç±»åˆ«æœ‰å›ºå®šæ•°é‡çš„æ ·æœ¬

    Args:
        base_dir: æ•°æ®é›†æ ¹ç›®å½•
        test_samples_per_class: æ¯ä¸ªç±»åˆ«åœ¨æµ‹è¯•é›†ä¸­çš„æ ·æœ¬æ•°é‡
        train_ratio: å‰©ä½™æ•°æ®ä¸­è®­ç»ƒé›†çš„æ¯”ä¾‹
        random_seed: éšæœºç§å­

    Returns:
        train_paths, val_paths, test_paths: ä¸‰ä¸ªæ•°æ®é›†çš„è·¯å¾„åˆ—è¡¨
    """

    # è®¾ç½®éšæœºç§å­
    random.seed(random_seed)

    # æŒ‰ç±»åˆ«æ”¶é›†å›¾åƒè·¯å¾„
    categories = ['oil', 'scratch', 'stain']
    category_paths = defaultdict(list)

    print("ğŸ“Š æ”¶é›†å„ç±»åˆ«æ•°æ®...")
    for category in categories:
        category_folder = os.path.join(base_dir, category)
        if os.path.exists(category_folder):
            for image_file in os.listdir(category_folder):
                if image_file.lower().endswith(('.jpg', '.jpeg')):
                    category_paths[category].append(os.path.join(category_folder, image_file))

    # æ˜¾ç¤ºå„ç±»åˆ«æ•°æ®ç»Ÿè®¡
    for category in categories:
        print(f"  {category}: {len(category_paths[category])} å¼ å›¾ç‰‡")

    # æ£€æŸ¥æ¯ä¸ªç±»åˆ«æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
    for category in categories:
        if len(category_paths[category]) < test_samples_per_class:
            raise ValueError(f"ç±»åˆ« '{category}' åªæœ‰ {len(category_paths[category])} å¼ å›¾ç‰‡ï¼Œ"
                             f"æ— æ³•æä¾› {test_samples_per_class} å¼ æµ‹è¯•æ ·æœ¬")

    # ä¸ºæ¯ä¸ªç±»åˆ«åˆ†åˆ«åˆ’åˆ†æ•°æ®
    train_paths = []
    val_paths = []
    test_paths = []

    print(f"\nğŸ¯ æŒ‰ç±»åˆ«åˆ’åˆ†æ•°æ® (æµ‹è¯•é›†æ¯ç±» {test_samples_per_class} å¼ )...")

    for category in categories:
        category_images = category_paths[category].copy()
        random.shuffle(category_images)  # éšæœºæ‰“ä¹±

        # å…ˆå–å‡ºæµ‹è¯•é›†
        test_images = category_images[:test_samples_per_class]
        remaining_images = category_images[test_samples_per_class:]

        # å‰©ä½™æ•°æ®æŒ‰æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        remaining_count = len(remaining_images)
        train_count = int(remaining_count * train_ratio)

        train_images = remaining_images[:train_count]
        val_images = remaining_images[train_count:]

        # æ·»åŠ åˆ°æ€»åˆ—è¡¨
        train_paths.extend(train_images)
        val_paths.extend(val_images)
        test_paths.extend(test_images)

        print(f"  {category}:")
        print(f"    è®­ç»ƒé›†: {len(train_images)} å¼ ")
        print(f"    éªŒè¯é›†: {len(val_images)} å¼ ")
        print(f"    æµ‹è¯•é›†: {len(test_images)} å¼ ")

    # æœ€ç»ˆæ‰“ä¹±å„æ•°æ®é›†
    random.shuffle(train_paths)
    random.shuffle(val_paths)
    random.shuffle(test_paths)

    # ç»Ÿè®¡ä¿¡æ¯
    total_samples = len(train_paths) + len(val_paths) + len(test_paths)
    print(f"\nğŸ“ˆ æœ€ç»ˆæ•°æ®é›†ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†: {len(train_paths)} å¼  ({len(train_paths) / total_samples * 100:.1f}%)")
    print(f"  éªŒè¯é›†: {len(val_paths)} å¼  ({len(val_paths) / total_samples * 100:.1f}%)")
    print(f"  æµ‹è¯•é›†: {len(test_paths)} å¼  ({len(test_paths) / total_samples * 100:.1f}%)")
    print(f"  æ€»è®¡: {total_samples} å¼ ")

    # éªŒè¯æµ‹è¯•é›†çš„ç±»åˆ«åˆ†å¸ƒ
    print(f"\nâœ… æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒéªŒè¯:")
    test_category_count = defaultdict(int)
    for path in test_paths:
        category = os.path.basename(os.path.dirname(path))
        test_category_count[category] += 1

    for category in categories:
        count = test_category_count[category]
        print(f"  {category}: {count} å¼  {'âœ“' if count == test_samples_per_class else 'âœ—'}")

    return train_paths, val_paths, test_paths


# ================================
# è¯„ä¼°å’Œåˆ†æå‡½æ•°ï¼ˆç›´æ¥æ•´åˆï¼‰
# ================================

def evaluate_classification_model(model, test_loader, device):
    """è¯„ä¼°åˆ†ç±»æ¨¡å‹"""
    model.eval()

    true_labels = []
    predicted_labels = []
    all_image_paths = []
    prediction_probs = []

    categories = ['oil', 'scratch', 'stain']
    category_names = ['æ²¹æ±¡', 'åˆ’ç—•', 'æ–‘ç‚¹']

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="è¯„ä¼°ä¸­"):
            images, labels, image_paths = batch
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)

            # è·å–æ¦‚ç‡å’Œé¢„æµ‹ç»“æœ
            probs = torch.softmax(outputs, dim=1)
            _, predicted = torch.max(outputs.data, 1)

            true_labels.extend(labels.cpu().numpy())
            predicted_labels.extend(predicted.cpu().numpy())
            all_image_paths.extend(image_paths)
            prediction_probs.extend(probs.cpu().numpy())

    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = accuracy_score(true_labels, predicted_labels)

    print("\n" + "=" * 60)
    print("ğŸ¯ å›¾åƒåˆ†ç±»è¯„ä¼°ç»“æœ")
    print("=" * 60)
    print(f"æ€»ä½“å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy * 100:.2f}%)")

    # è¯¦ç»†æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(true_labels, predicted_labels, target_names=category_names))

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(true_labels, predicted_labels)
    print("\næ··æ·†çŸ©é˜µ:")
    print("çœŸå®\\é¢„æµ‹", "  ".join([f"{name:>6}" for name in category_names]))
    for i, true_name in enumerate(category_names):
        row_str = f"{true_name:>8}: "
        row_str += "  ".join([f"{cm[i, j]:>6}" for j in range(len(category_names))])
        print(row_str)

    return accuracy, true_labels, predicted_labels, all_image_paths, prediction_probs


def analyze_test_results(test_paths, true_labels, predicted_labels):
    """åˆ†ææµ‹è¯•é›†ç»“æœçš„è¯¦ç»†æƒ…å†µ"""

    categories = ['oil', 'scratch', 'stain']
    category_names = ['æ²¹æ±¡', 'åˆ’ç—•', 'æ–‘ç‚¹']

    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•é›†è¯¦ç»†åˆ†æ (æ¯ç±»30å¼ )")
    print("=" * 60)

    # æŒ‰ç±»åˆ«åˆ†æå‡†ç¡®ç‡
    for i, (category, category_name) in enumerate(zip(categories, category_names)):
        class_true = [j for j, label in enumerate(true_labels) if label == i]
        class_predicted = [predicted_labels[j] for j in class_true]
        class_correct = sum(1 for pred in class_predicted if pred == i)

        accuracy = class_correct / len(class_true) if len(class_true) > 0 else 0

        print(f"\n{category_name} ({category}):")
        print(f"  æµ‹è¯•æ ·æœ¬: {len(class_true)} å¼ ")
        print(f"  é¢„æµ‹æ­£ç¡®: {class_correct} å¼ ")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy * 100:.2f}%)")

        # æ˜¾ç¤ºé”™è¯¯é¢„æµ‹çš„è¯¦æƒ…
        if class_correct < len(class_true):
            error_count = len(class_true) - class_correct
            print(f"  é”™è¯¯é¢„æµ‹: {error_count} å¼ ")

            # ç»Ÿè®¡é”™è¯¯é¢„æµ‹åˆ°å“ªäº›ç±»åˆ«
            error_distribution = defaultdict(int)
            for pred in class_predicted:
                if pred != i:
                    error_distribution[pred] += 1

            for wrong_class, count in error_distribution.items():
                wrong_name = category_names[wrong_class]
                print(f"    è¯¯åˆ¤ä¸º{wrong_name}: {count} å¼ ")


def save_results_to_csv(image_paths, true_labels, predicted_labels, prediction_probs,
                        output_file="enhanced_classification_results.csv"):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°CSVæ–‡ä»¶"""

    categories = ['oil', 'scratch', 'stain']
    category_names = ['æ²¹æ±¡', 'åˆ’ç—•', 'æ–‘ç‚¹']

    try:
        with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            # å†™å…¥è¡¨å¤´
            header = ['å›¾ç‰‡è·¯å¾„', 'çœŸå®ç±»åˆ«', 'é¢„æµ‹ç±»åˆ«', 'æ˜¯å¦æ­£ç¡®', 'ç½®ä¿¡åº¦']
            header.extend([f'{name}_æ¦‚ç‡' for name in category_names])
            writer.writerow(header)

            # å†™å…¥æ•°æ®
            for i, (img_path, true_label, pred_label, probs) in enumerate(
                    zip(image_paths, true_labels, predicted_labels, prediction_probs)
            ):
                is_correct = "æ­£ç¡®" if true_label == pred_label else "é”™è¯¯"
                confidence = probs[pred_label]

                row = [
                    os.path.basename(img_path),
                    category_names[true_label],
                    category_names[pred_label],
                    is_correct,
                    f"{confidence:.4f}"
                ]

                # æ·»åŠ å„ç±»åˆ«æ¦‚ç‡
                for prob in probs:
                    row.append(f"{prob:.4f}")

                writer.writerow(row)

        print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    except Exception as e:
        print(f"âŒ ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")


def plot_training_curves(train_losses, val_accuracies, save_path="enhanced_training_curves.png"):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""

    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # è®­ç»ƒæŸå¤±æ›²çº¿
    ax1.plot(train_losses, 'b-', label='è®­ç»ƒæŸå¤±')
    ax1.set_title('è®­ç»ƒæŸå¤±å˜åŒ–')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)

    # éªŒè¯å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(val_accuracies, 'r-', label='éªŒè¯å‡†ç¡®ç‡')
    ax2.set_title('éªŒè¯å‡†ç¡®ç‡å˜åŒ–')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

    print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")


# ================================
# ä¸»å‡½æ•° - æ•´åˆæ‰€æœ‰æ”¹è¿›
# ================================

def main():
    """ä¸»è®­ç»ƒå‡½æ•° - é›†æˆæ‰€æœ‰ä¼˜åŒ–ç­–ç•¥"""

    # è®¾ç½®è®¾å¤‡å’Œéšæœºç§å­
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

    # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)

    # æ•°æ®è·¯å¾„
    base_dir = "MSD-US/train_set"

    # æ•°æ®é›†åˆ’åˆ†
    train_paths, val_paths, test_paths = create_balanced_dataset_split(
        base_dir=base_dir,
        test_samples_per_class=30,
        train_ratio=0.8,
        random_seed=42
    )

    # è·å–ä¸“é—¨çš„æ•°æ®å¢å¼ºç­–ç•¥
    train_transform = get_defect_focused_augmentations(is_training=True)
    val_transform = get_defect_focused_augmentations(is_training=False)

    print("ğŸ¯ åˆ›å»ºä¸“æ³¨ç¼ºé™·åŒºåŸŸçš„æ•°æ®é›†...")

    # åˆ›å»ºå¢å¼ºçš„æ•°æ®é›†
    train_dataset = DefectFocusedDataset(train_paths, train_transform, focus_on_defects=True)
    val_dataset = DefectFocusedDataset(val_paths, val_transform, focus_on_defects=True)
    test_dataset = DefectFocusedDataset(test_paths, val_transform, focus_on_defects=True)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=16,  # å‡å°batch sizeä»¥é€‚åº”æ›´å¤æ‚çš„æ¨¡å‹
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        drop_last=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=16,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=16,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )

    print(f"ğŸ“¦ å¢å¼ºæ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    print(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)} æ‰¹")
    print(f"  éªŒè¯æ‰¹æ¬¡: {len(val_loader)} æ‰¹")
    print(f"  æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)} æ‰¹")

    # åˆ›å»ºå¢å¼ºæ¨¡å‹
    model = EnhancedResNetClassifier(num_classes=3, pretrained=True, use_attention=True)
    model = model.to(device)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ¯ å¢å¼ºæ¨¡å‹å‚æ•°é‡: {trainable_params:,} / {total_params:,}")

    # è®­ç»ƒæ¨¡å‹
    print("\nğŸš€ å¼€å§‹å¢å¼ºè®­ç»ƒ...")
    train_losses, val_accuracies, best_accuracy = train_enhanced_classification(
        model, train_loader, val_loader, device, num_epochs=50
    )

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_curves(train_losses, val_accuracies, "enhanced_training_curves.png")

    # åŠ è½½æœ€ä½³æ¨¡å‹å¹¶æµ‹è¯•
    print("\nğŸ” åŠ è½½æœ€ä½³å¢å¼ºæ¨¡å‹è¿›è¡Œæµ‹è¯•...")
    checkpoint = torch.load("best_enhanced_model.pth")
    model.load_state_dict(checkpoint['model_state_dict'])

    # æµ‹è¯•è¯„ä¼°
    accuracy, true_labels, predicted_labels, image_paths, prediction_probs = evaluate_classification_model(
        model, test_loader, device
    )

    # è¯¦ç»†åˆ†æ
    analyze_test_results(test_paths, true_labels, predicted_labels)

    # ä¿å­˜ç»“æœ
    save_results_to_csv(
        image_paths, true_labels, predicted_labels, prediction_probs,
        "enhanced_classification_results.csv"
    )

    # å¯è§†åŒ–æ³¨æ„åŠ›å›¾
    print("\nğŸ¨ ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–å›¾...")
    visualize_attention_maps(model, test_loader, device)

    # æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ‰ å¢å¼ºè®­ç»ƒå®Œæˆ!")
    print("=" * 60)
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")

    files = [
        "best_enhanced_model.pth",
        "enhanced_training_curves.png",
        "enhanced_classification_results.csv",
        "attention_visualizations/"
    ]

    for file in files:
        if os.path.exists(file):
            print(f"  âœ… {file}")
        else:
            print(f"  âŒ {file} (æœªç”Ÿæˆ)")

    print(f"\nğŸ¯ æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy * 100:.2f}%)")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.4f} ({best_accuracy * 100:.2f}%)")

    print("\nğŸ”§ ä¸»è¦ä¼˜åŒ–ç­–ç•¥:")
    print("  âœ… ç¼ºé™·ç‰¹å¾å¢å¼ºé¢„å¤„ç†")
    print("  âœ… CBAMæ³¨æ„åŠ›æœºåˆ¶")
    print("  âœ… ç„¦ç‚¹æŸå¤±å‡½æ•°")
    print("  âœ… ä¸“é—¨çš„æ•°æ®å¢å¼ºç­–ç•¥")
    print("  âœ… æ³¨æ„åŠ›å¯è§†åŒ–")
    print("  âœ… æ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦")


if __name__ == "__main__":
    try:
        main()

    except KeyboardInterrupt:
        print("\nâ¹ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()